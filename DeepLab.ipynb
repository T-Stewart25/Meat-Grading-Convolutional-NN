{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3275437a-c5cb-49c4-9ebe-83debee4948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import os\n",
    "#Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eb2c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(root_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e69d1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 48, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(48)\n",
    "        self.conv2 = nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, low_level_features):\n",
    "        low_level_features = self.conv1(low_level_features)\n",
    "        low_level_features = self.bn1(low_level_features)\n",
    "        low_level_features = self.relu(low_level_features)\n",
    "        \n",
    "        x = F.interpolate(x, size=low_level_features.size()[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat((x, low_level_features), dim=1)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "646cd9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "        # Define dilated convolutions with different dilation rates\n",
    "        self.conv1x1_1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.conv3x3_6 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=6, dilation=6)\n",
    "        self.conv3x3_12 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=12, dilation=12)\n",
    "        self.conv3x3_18 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=18, dilation=18)\n",
    "\n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Convolution to adjust dimensions\n",
    "        self.conv1x1_adjust = nn.Conv2d(in_channels * 4, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply dilated convolutions with different dilation rates\n",
    "        out_1x1 = self.conv1x1_1(x)\n",
    "        out_3x3_6 = self.conv3x3_6(x)\n",
    "        out_3x3_12 = self.conv3x3_12(x)\n",
    "        out_3x3_18 = self.conv3x3_18(x)\n",
    "\n",
    "        # Global average pooling\n",
    "        global_pool = self.global_avg_pool(x)\n",
    "        global_pool = torch.cat([global_pool] * x.size()[2:], dim=2)\n",
    "        global_pool = torch.cat([global_pool] * x.size()[3:], dim=3)\n",
    "\n",
    "        # Concatenate the outputs of all paths\n",
    "        out_concat = torch.cat([out_1x1, out_3x3_6, out_3x3_12, out_3x3_18, global_pool], dim=1)\n",
    "\n",
    "        # Apply 1x1 convolution to adjust dimensions\n",
    "        out_adjust = self.conv1x1_adjust(out_concat)\n",
    "\n",
    "        return out_adjust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6676924-7eae-45e8-afc8-a1b527905b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV3Plus(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DeepLabV3Plus, self).__init__()\n",
    "        # Define the backbone (e.g., ResNet)\n",
    "        self.backbone = models.resnet50(pretrained=False)\n",
    "        # Modify the backbone's final layers to match the output stride required for DeepLabv3+\n",
    "        self.backbone.layer4[0].conv2 = nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
    "        # Adjust the strides or use dilated convolutions to achieve the desired output stride\n",
    "        # Define ASPP module\n",
    "        self.aspp = ASPP(2048, 256)\n",
    "        # Define decoder module\n",
    "        self.decoder = Decoder(256, num_classes)\n",
    "        # Upsample the output\n",
    "        self.upsample = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        # Final convolution to get the segmentation map\n",
    "        self.final_conv = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass\n",
    "        x_size = x.size()\n",
    "        features = self.backbone(x)\n",
    "        aspp_output = self.aspp(features['out'])\n",
    "        decoder_output = self.decoder(aspp_output, features['low_level'])\n",
    "        upsampled_output = self.upsample(decoder_output)\n",
    "        segmentation_map = self.final_conv(upsampled_output)\n",
    "        return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3446bdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),  \u001b[38;5;66;03m# Resize the image to 256x256 pixels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()            \u001b[38;5;66;03m# Convert the image to a PyTorch tensor\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create an instance of the ImageDataset class\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Optionally, you can also use PyTorch's ImageFolder class which automatically loads images from subfolders\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# dataset = ImageFolder(root=root_dir, transform=transform)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create a DataLoader to iterate over the dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mImageDataset.__init__\u001b[0;34m(self, root_dir, transform)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir \u001b[38;5;241m=\u001b[39m root_dir\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Train'"
     ]
    }
   ],
   "source": [
    "# Define the path to the directory containing your images\n",
    "root_dir = \"../Train\"\n",
    "\n",
    "# Define transformations to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256 pixels\n",
    "    transforms.ToTensor()            # Convert the image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "# Create an instance of the ImageDataset class\n",
    "dataset = ImageDataset(root_dir, transform=transform)\n",
    "\n",
    "# Optionally, you can also use PyTorch's ImageFolder class which automatically loads images from subfolders\n",
    "# dataset = ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# Create a DataLoader to iterate over the dataset\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215a570-cb8d-4d84-bf7f-8dba68618e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasstewart/micromamba/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/thomasstewart/micromamba/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "model = DeepLabV3Plus(num_classes)\n",
    "# Define your optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define your loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abdf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
